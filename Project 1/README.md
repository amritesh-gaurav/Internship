## Purpose
Educators were facing a challenge to be able to get correct feedback from students so that they could get a proper idea of how effective their lectures were and what they could do to improve the quality of lectures and get better student retention during the lecture.

## Methodology

We have been to multiple online meetings, especially during the pandemic, be it in Google Meet or Zoom or Microsoft Teams etc. One unique attribute of all these platforms is the unique division of people in different tiles. Each person has his/her own tile of the live stream, and the size of tile reduces or increases based on the number of people in that particular meeting. We wanted a system focused on each and every tile and assessing their emotional state at each instant of time. 

Initially, we tried to split each frame into different pieces based on the height and width specified for each splitted image, but it could not be implemented for a batch having different number of participants.

We then started with training a model which creates a perfect bounding box around each tile, irrespective of the number of attendees in the meeting. We started collecting data and used YOLOv5 custom object training using ROBOFLOW for training our model. We took a recording of a lecture and split each frame of the video to get our training samples. We also downloaded a few images from the internet to make the sample diverse. We cherry-picked the images to make sure that the data sample is good enough to handle multiple scenarios faced during a live lecture. In the final data, we had 50 distinct images with around 20-30 labels per image with some having close to 50 labels in a single image. We then labeled the data, and the training-test-validation split-up was performed with the help of ROBOFLOW. We applied a few pre-processing steps and then exported the data in YOLOv5 PyTorch txt version. With this we wanted to create a bounding box around the tiles during a live lecture. The results were pretty accurate for such a small dataset. Each box was created by the trained model with >85% confidence. 

For the next step, we began with training our model to split between the live tiles and the tiles with no video stream. By live tiles, we mean those tiles where there is a live video stream of any participant of the meeting. Now to separate the tiles with a live capture from the tiles where there was no video capture, we used the light intensity attribute. We figured out that the tiles with live capturing had more light intensity than the ones with no capture. With trial-and-error, we figured out the threshold to be used to separate the live tiles and the tiles with no video stream.

At the end of our second step, we have successfully created a bounding box around the tiles and have a way to distinctively capture only the live tiles. Using built-in functions of YOLOv5, crop and save, we were able to successfully save the live captures of each participant at a specific moment and save it in a directory for further investigation of the emotional state of each individual at any specific instant.
