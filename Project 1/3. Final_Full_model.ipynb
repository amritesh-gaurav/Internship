{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd yolov5\n",
    "#%pip install -qr requirements.txt  # install\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "#display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference  With Trained Weights\n",
    "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow or test a video file of your own.\n",
    "We tested the previously trained model using a test video and saved the cropped splits at `'runs/detect/exp/crops'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing a video file\n",
    "!python detect.py --weights runs/train/exp/weights/best.pt --conf 0.8 --source ../Meet_test.m4v --save-crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video containing the bounding box can be downloaded from `'runs/detect/exp'` or whatever directory is shown after execution of the above command.\n",
    "i.e\n",
    "```\n",
    "Speed: 0.4ms pre-process, 11.5ms inference, 1.0ms NMS per image at shape (1, 3, 640, 640)\n",
    "Results saved to runs/detect/exp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing model on the test images\n",
    "!python detect.py --weights runs/train/exp/weights/best.pt --conf 0.8 --source ../test/images --save-crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display Light(Live Capturing) Images\n",
    "# Each of the cropped images was then to be classified between the live tiles and the tiles with no video stream.\n",
    "# By live tiles, we mean those tiles where there is a live video stream of any participant of the meeting.\n",
    "# Now we separated the tiles with a live capture from the tiles where there was no video capture based on the amount of luminance present in each split image.\n",
    "# We figured out that the tiles with live capturing had more light intensity than the ones with no capture. \n",
    "# With trial-and-error, we figured out the threshold(70) to be used to separate the live tiles and the tiles with no video stream. \n",
    "\n",
    "import glob\n",
    "import imageio.v3 as imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def img_estim(img, thrshld=70):\n",
    "            is_light = np.mean(img) > thrshld\n",
    "            return True if is_light else False\n",
    "\n",
    "\n",
    "for imageName in glob.glob('/Full_Model/yolov5/runs/detect/exp/crops/live/*.jpg'): #assuming JPG\n",
    "    t = imageio.imread(imageName)\n",
    "    if ( img_estim(t, 70) == True):\n",
    "        print(imageName + \" \\t\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45e6d53874419d5ee069b316505fbf446d1495aeccb32ce5c14ec9b4506a392b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
