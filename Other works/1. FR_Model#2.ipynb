{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Identification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rSYXyCaYxVbllebRKPlAh1nCgTyOoEc4",
      "authorship_tag": "ABX9TyPmr+voqS35f5M0hEQJzCDD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amritesh-gaurav/Face-Detection/blob/main/Face_Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M-3M9dIRUey9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/Intern Material/OpenCV-Python-Series-master/src/cascades/data/haarcascade_frontalface_default.xml')\n",
        "eye_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/Intern Material/OpenCV-Python-Series-master/src/cascades/data/haarcascade_eye.xml')\n",
        "smile_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/Intern Material/OpenCV-Python-Series-master/src/cascades/data/haarcascade_smile.xml')\n",
        "\n",
        "\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "recognizer.read(\"./recognizers/face-trainner.yml\")\n",
        "\n",
        "labels = {\"person_name\": 1}\n",
        "with open(\"pickles/face-labels.pickle\", 'rb') as f:\n",
        "\tog_labels = pickle.load(f)\n",
        "\tlabels = {v:k for k,v in og_labels.items()}\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while(True):\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "    gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5)\n",
        "    for (x, y, w, h) in faces:\n",
        "    \t#print(x,y,w,h)\n",
        "    \troi_gray = gray[y:y+h, x:x+w] #(ycord_start, ycord_end)\n",
        "    \troi_color = frame[y:y+h, x:x+w]\n",
        "\n",
        "    \t# recognize? deep learned model predict keras tensorflow pytorch scikit learn\n",
        "    \tid_, conf = recognizer.predict(roi_gray)\n",
        "    \tif conf>=4 and conf <= 85:\n",
        "    \t\t#print(5: #id_)\n",
        "    \t\t#print(labels[id_])\n",
        "    \t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    \t\tname = labels[id_]\n",
        "    \t\tcolor = (255, 255, 255)\n",
        "    \t\tstroke = 2\n",
        "    \t\tcv2.putText(frame, name, (x,y), font, 1, color, stroke, cv2.LINE_AA)\n",
        "\n",
        "    \timg_item = \"7.png\"\n",
        "    \tcv2.imwrite(img_item, roi_color)\n",
        "\n",
        "    \tcolor = (255, 0, 0) #BGR 0-255 \n",
        "    \tstroke = 2\n",
        "    \tend_cord_x = x + w\n",
        "    \tend_cord_y = y + h\n",
        "    \tcv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
        "    \t#subitems = smile_cascade.detectMultiScale(roi_gray)\n",
        "    \t#for (ex,ey,ew,eh) in subitems:\n",
        "    \t#\tcv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
        "    # Display the resulting frame\n",
        "    cv2.imshow('frame',frame)\n",
        "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# When everything done, release the capture\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ]
}